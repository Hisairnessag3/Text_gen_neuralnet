——————————Text_gen_Readme——————————

This is a LSTM(Long short-term memory) Recurrent Neural Net that is used to generate text from Mark Twain’s “Adventures of Huckleberry Finn”. Dropout layer, and softmax classification. Built using keras and tensorflow backend. 

Total Characters: 566,529
Total Vocal: 51

Total Patterns: 566,429

Total Epochs (Training): 20
Batch size:128 

Generated text: why, the way the way toe to to the sayt and soee the sayt and see the war ho the way he was so the siar to the pine oo the siver, and he was all right and sa


Notes on model and output: The results easily could of been improved in a number of ways. Simply running more epochs is one of the first things that comes to mind(100+). Also, building out the model to include more layers and memory would yield better results. 

There is a number of additional ways the model could be fine-tuned for significantly lower loss. Testing adjustments in batch-size, dropout layers, and the parameters of the LSTM layers could all lead to a lower loss.  
